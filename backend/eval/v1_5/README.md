# Feb v1.5 Evaluation Suite — CLI Reference

## Quick Start

```bash
cd backend
npx tsx eval/v1_5/manifest/generate_manifest.js    # one-time manifest generation
npx tsx eval/v1_5/runners/run_v1_5.js --dataset core30
```

## Commands

### Run Evaluation

```bash
npx tsx eval/v1_5/runners/run_v1_5.js --dataset <name> [options]
```

| Flag | Description |
|------|-------------|
| `--dataset <name>` | **Required.** Dataset to evaluate: `core30`, `hard60`, `fuzz200`, `realistic50`, or `all` |
| `--experiment <name>` | Activate an experiment (repeatable) |
| `--compare <path>` | Compare results with a previous report JSON |
| `--list-experiments` | List all available experiments and exit |
| `--verbose` | Enable detailed per-case logging |

### Examples

```bash
# Evaluate core30
npx tsx eval/v1_5/runners/run_v1_5.js --dataset core30

# Evaluate all datasets
npx tsx eval/v1_5/runners/run_v1_5.js --dataset all

# Run with experiment
npx tsx eval/v1_5/runners/run_v1_5.js --dataset core30 --experiment amount_v2

# Compare with previous baseline
npx tsx eval/v1_5/runners/run_v1_5.js --dataset core30 --compare eval/v1_5/reports/LATEST_core30.json

# Multiple experiments
npx tsx eval/v1_5/runners/run_v1_5.js --dataset core30 --experiment amount_v2 --experiment name_v2
```

### Generate Manifest

```bash
npx tsx eval/v1_5/manifest/generate_manifest.js
```

Run this once after creating/modifying datasets. The manifest captures SHA-256 checksums and case counts for all datasets. Evaluations fail if datasets no longer match the manifest.

## Architecture (5 Layers)

```
Layer 1: Dataset Integrity     — SHA-256 manifest validation before every run
Layer 2: Field-Level Metrics   — Per-field accuracy, confusion matrices, partial matches
Layer 3: Scoring Isolation     — STRUCTURAL | FULL_STRICT | URGENCY_ONLY modes
Layer 4: Experiment Engine     — Scoped env overrides without touching core engine
Layer 5: Failure Buckets       — 17 granular root-cause classifications
```

### Layer 1 — Dataset Integrity

- Manifest at `eval/v1_5/manifest/dataset_manifest.json`
- Generated by `generate_manifest.js`
- Validated before every eval run
- **Evaluation aborts** if any dataset hash mismatches

### Layer 2 — Field-Level Metrics

Reports per-field accuracy independently:
- **Name**: accuracy, partial matches, failures
- **Category**: accuracy, confusion matrix
- **Urgency**: accuracy, under/over counts
- **Amount**: accuracy, null counts, tolerance failures

### Layer 3 — Scoring Isolation

Three independent scoring modes prevent one field from masking another:

| Mode | Fields Scored | Purpose |
|------|--------------|---------|
| **STRUCTURAL** | name + category + amount | Extraction quality without urgency noise |
| **FULL_STRICT** | all 4 fields | Traditional strict pass (≥0.95) |
| **URGENCY_ONLY** | urgency only | Binary urgency accuracy |

### Layer 4 — Experiment Engine

Experiments modify environment variables or config without touching production code.

Available experiments:
- `amount_v2` — Extended spoken numbers
- `name_v2` — Extended name regex patterns
- `urgency_threshold_032` — Lower HIGH threshold to 0.32
- `urgency_threshold_028` — Lower HIGH threshold to 0.28
- `category_emergency` — Emergency keyword list
- `strict_name_reject` — Enhanced name filtering
- `amount_tolerance_005` — Tighten to 5%
- `amount_tolerance_015` — Loosen to 15%
- `no_enhancements` — Disable all enhancement flags

Custom experiments: add a JSON file to `eval/v1_5/experiments/`.

### Layer 5 — Failure Bucket Intelligence

Every failure maps to **exactly one** bucket from 17 categories:

**Amount** (5): `spoken_number_failure`, `partial_match_override`, `null_extraction`, `tolerance_exceeded`, `false_positive`

**Name** (5): `intro_pattern_missing`, `reject_filter_failure`, `partial_capture`, `lowercase_capture`, `null_extraction`

**Category** (3): `vocabulary_gap`, `priority_conflict`, `multi_signal_conflict`

**Urgency** (4): `threshold_miss`, `signal_absent`, `multiplier_override`, `engine_conflict`

## Report Format

Reports save to `eval/v1_5/reports/` as JSON with structure:

```json
{
  "metadata": {
    "engine_hash": "...",
    "dataset_manifest_hash": "...",
    "experiment_flags": [],
    "run_time_ms": 75,
    "timestamp": "ISO-8601",
    "datasets_evaluated": ["core30"],
    "total_cases": 30,
    "version": "feb_v1_5"
  },
  "summary": {
    "structural_score": { "pass_rate": 0.2667, "passes": 8, "total": 30 },
    "urgency_score": { "accuracy": 0.5333, "correct": 16, "total": 30 },
    "strict_score": { "pass_rate": 0.1000, "passes": 3, "total": 30 },
    "acceptable_score": { "pass_rate": 0.1000, "passes": 3, "total": 30 }
  },
  "field_metrics": { ... },
  "failure_buckets": { ... },
  "dataset_breakdown": { ... }
}
```

A `LATEST_<dataset>.json` symlink is always updated for easy `--compare` usage.

## Comparison Feature

The `--compare` flag loads a previous report and shows deltas for:
- Score changes (structural, urgency, strict, acceptable)
- Field-level accuracy changes
- Failure bucket count changes (improvements first)
- Per-dataset breakdowns

**Comparison is refused** if `dataset_manifest_hash` differs between runs.

## Reproducibility Guarantees

1. **Deterministic**: Same engine + same dataset = same results
2. **Engine hash**: SHA-256 of rulesEngine.ts + urgencyEngine.ts + amountEngine.ts
3. **Manifest hash**: SHA-256 of all dataset file checksums
4. **Zero OpenAI**: All runs enforce ZERO_OPENAI_MODE + block HTTP/HTTPS
5. **Experiment isolation**: env backup/restore ensures no state leakage

## File Layout

```
backend/eval/v1_5/
├── manifest/
│   ├── dataset_manifest.json      # Generated checksums
│   ├── generate_manifest.js       # Manifest generator
│   └── validate_integrity.js      # Pre-run validation
├── runners/
│   ├── run_v1_5.js               # Main orchestrator
│   ├── scoring_isolation.js       # Layer 3 scoring
│   ├── experiment_engine.js       # Layer 4 experiments
│   └── baseline_comparator.js     # --compare feature
├── diagnostics/
│   ├── field_metrics.js           # Layer 2 per-field metrics
│   └── failure_buckets.js         # Layer 5 classification
├── reports/                       # Auto-generated report JSONs
├── experiments/                   # Custom experiment JSON files
├── eval_logs/                     # Integrity error logs
└── README.md                      # This file
```
